---
title: "Using allFit() with (g)lmer"
author: "Josh Nugent"
output:
  html_document:
    toc: true
    toc_depth: 1
    toc_float: true
    number_sections: false  ## if you want number sections at each table header
---

# TL;DR

To automatically iterate through all possible (g)lmer optimizers and find one that converges, use code similar to what's below:

```
model <- lmer(y ~ arm + (1 + arm | clustid), data = dat)
diff_optims <- allFit(model, maxfun = 1e5, parallel = 'multicore', ncpus = detectCores())
is.OK <- sapply(diff_optims, is, "merMod")
diff_optims.OK <- diff_optims[is.OK]
lapply(diff_optims.OK,function(x) x@optinfo$conv$lme4$messages)

convergence_results <- lapply(diff_optims.OK,function(x) x@optinfo$conv$lme4$messages)
working_indices <- sapply(convergence_results, is.null)

if(sum(working_indices)==0){
  print("No algorithms from allFit converged.")
  print("You may still be able to use the results, but proceed with extreme caution.")
  first_fit <- NULL
} else {
  first_fit <- diff_optims[working_indices][[1]]
}
first_fit
```

If that doesn't work, see the **Power User** section below.

Also, allFit() sometimes has trouble finding the input data for the models when it is nested in another function. See discussion
[here](https://stackoverflow.com/questions/56893715/lme4-allfit-giving-confusing-results-when-wrapped-in-a-function)
and / or contact Josh if you want some ideas for workarounds.

# Using allFit()

## Introduction
I frequently encounter onvergence problems when fitting mixed models. The goal of this vignette is to show how to use allFit() in the lme4 package to quickly test different optimizers in search of convergence.

This document draws heavily on posts by 
[Steven V. Miller](http://svmiller.com/blog/2018/06/mixed-effects-models-optimizer-checks/), 
[unknown author](https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html), and
[unknown author](https://cran.r-project.org/web/packages/lme4/vignettes/lmerperf.html), many thanks!

```{r setup, message = FALSE, warning=FALSE, cache=TRUE}
library(tidyverse)
library(lme4)
library(optimx)
library(parallel)
library(minqa)
```

## Our hypothetical test case

We assume a linear mixed model (LMM) approach to analyzing data from a two-arm cluster-randomized trial where the cluster-level variance is different in the two arms. We will simulate data using the code below. The variable **nsub** is the number of subjects per cluster and **nclust** is the number of clusters per arm, so your total sample size will be $nsub*nclust*2$. The **betas** give the mean responses in each arm (we'll fix the control arm at zero), **sig_b2** is a vector of cluster-level variance in each arm, and **sigma_residual** is the within-cluster variance, assumed to the the same across each arm. (To relax the sigma_residual assumption, you can use the nlme package with different weights by arm.)

```{r simulator_setup, message = FALSE, warning=FALSE, cache=TRUE}
sim <- function(nsub = 2, nclust = 3, sigma_residual2 = 1,  sig_b2 = c(.3, 10), betas = c(0,1)){
  narm <- 2
  n <- nsub * nclust * narm
  y <- rep_len(NA, n)
  arm <- rep(0:(narm-1), each = nsub*nclust)
  arm_factor <- as.factor(arm)
  clustid <- rep(1:(nclust*narm), each = nsub)
  clustRElist <- rnorm(narm*nclust, mean = 0, sd = rep(sqrt(sig_b2), each = nclust))
  clustRE <- rep(clustRElist, each = nsub)
  sig_b2 <- rep(sig_b2, each = nclust*nsub)
  error <- rnorm(n, mean = 0, sd = sqrt(sigma_residual2))
  beta <- rep(betas, each = nclust*nsub)
  y <- beta + clustRE + error
  output <- cbind.data.frame(arm, arm_factor, clustid, sig_b2, clustRE, error, beta, y)
  return(output)
}
set.seed(2)
dat <- sim(nsub = 100, nclust = 100, sigma_residual2 = .2,  sig_b2 = c(.3, 1), betas = c(0,1))
```

## Convergence fails with default settings

With our data in hand, let's fit a model that allows for different cluster-level variances in the two arms - we have a random intercept that is shared between both arms, then an additional amount of variance when arm = 1. When fitting this model, we get a convergence error. Not surprising, given the purpose of this vignette.

```{r cache=TRUE}
model <- lmer(y ~ arm + (1 + arm | clustid), data = dat)
```

## Finding an algorithm that works via allFit()

Using allFit(), we can see if different optimizers might give us better results. It uses parallel processing (if available) to run multiple fits at once, which really speeds things up. **maxfun** sets the maximum number of iterations, though I have had R freeze up sometimes when I make that value too high. In this case, 1e6 works.

```{r cache=TRUE}
ncores <- detectCores()
diff_optims <- allFit(model, maxfun = 1e5, parallel = 'multicore', ncpus = ncores)
is.OK <- sapply(diff_optims, is, "merMod")
diff_optims.OK <- diff_optims[is.OK]
lapply(diff_optims.OK,function(x) x@optinfo$conv$lme4$messages)
```

"NULL" is what we want to see here - that means there were no error or warning messages when that particular algorithm was run. While most algorithms did not converge, it looks like Nelder_Mead and nlminbwrap did. Yay! Further, these two algorithms are built in to lme4, so we do not require the optimx or nloptr packages (at least for now).  Let's check those and see if we are correct.

```{r cache=TRUE}
model2 <- lmer(y ~ arm + (1 + arm | clustid), data = dat,
               control = lmerControl(optimizer = "Nelder_Mead"))
model3 <- lmer(y ~ arm + (1 + arm | clustid), data = dat,
               control = lmerControl(optimizer = "nlminbwrap"))
```

No convergence warnings, yay!

If the best algorithm does come from the *optimx* or *nloptr* families, the syntax is a little different:
```{r cache=TRUE}
model4 <- lmer(y ~ arm + (1 + arm | clustid), data = dat,
               control = lmerControl(optimizer = "optimx",
                                     optCtrl = list(method = "L-BFGS-B", maxit = 1e6)))
model4 <- lmer(y ~ arm + (1 + arm | clustid), data = dat,
               control = lmerControl(optimizer = "nloptwrap",
                                     optCtrl = list(algorithm = "NLOPT_LN_BOBYQA", maxit = 1e6)))
```
We already knew that these algorithms wouldn't converge in this case, but in another situation those other algorithms might be needed.

Let's look at the model fits for the two algos that converged. Interestingly, the estimates of the 'arm' standard deviation and correlation with the intercept are quite different. However, when transforming those values into the original parameters, we find that they are two ways of getting to the same values.
```{r cache=TRUE}
convergence_results <- lapply(diff_optims.OK,function(x) x@optinfo$conv$lme4$messages)
working_indices <- sapply(convergence_results, is.null)
diff_optims[working_indices]
n_m_model <- diff_optims[working_indices][[1]]
nlminb_model <- diff_optims[working_indices][[2]]

# Extract parameters to check
sig_b2_n_m <- vector(length = 2)
random_effects_n_m <- as.data.frame(VarCorr(n_m_model)$clustid)
corrmatrix_n_m <- as.data.frame(VarCorr(n_m_model))$sdcor
sig_b2_n_m[1] <- random_effects_n_m[1,1]
sig_b2_n_m[2] <- random_effects_n_m[1,1] + random_effects_n_m[2,2] + 
  2*sqrt(random_effects_n_m[1,1])*sqrt(random_effects_n_m[2,2])*corrmatrix_n_m[3]

sig_b2_nlmin <- vector(length = 2)
random_effects_nlmin <- as.data.frame(VarCorr(nlminb_model)$clustid)
corrmatrix_nlmin <- as.data.frame(VarCorr(nlminb_model))$sdcor
sig_b2_nlmin[1] <- random_effects_nlmin[1,1]
sig_b2_nlmin[2] <- random_effects_nlmin[1,1] + random_effects_nlmin[2,2] + 
  2*sqrt(random_effects_nlmin[1,1])*sqrt(random_effects_nlmin[2,2])*corrmatrix_nlmin[3]

sig_b2_n_m
sig_b2_nlmin
```

## Automatically selecting a method that converges

It would be nice to automate this so that we could use whatever method converges and only be notified if *none* of them converge. Here's some code for that, with a bit of duplication from the last chunk:

```{r cache=TRUE}
convergence_results <- lapply(diff_optims.OK,function(x) x@optinfo$conv$lme4$messages)
working_indices <- sapply(convergence_results, is.null)
if(sum(working_indices)==0){
  print("No algorithms from allFit converged.")
  print("You may still be able to use the results, but proceed with extreme caution.")
  first_fit <- NULL
} else {
  first_fit <- diff_optims[working_indices][[1]]
}
first_fit
```


# Power User settings if allFit() doesn't work

One downside of using allFit() is that you can't customize convergence/optimizer settings - you have to use the defaults. **(NOTE: Recent updates to lme4 allow for control commands in allFit(), so some of the following may be irrelevant. See lme4 news / release notes for more information.)** Yet changing the settings (increasing the maximum number of iterations, lowering the stopping tolerances, etc) can greatly improve convergence, though sometimes at the cost of slower fitting time. Here is some simple code you can use that might help your algorithm converge.

## First Step: Change settings

Let's try the default optimizer with better settings and see if it works. Note that this is almost the same call in that first model we tried to fit, just with more specified options.
```{r cache = TRUE}
model5 <- lmer(y ~ arm + (1 + arm | clustid),
               data = dat,
               control = lmerControl(optCtrl = list(maxit = 1e9,
                                                    maxfun = 1e9,
                                                    xtol_abs = 1e-11,
                                                    ftol_abs = 1e-11,
                                                    maxeval = 1e9)))
summary(model5)
```

Yay! No errors or warnings. That was easy. But what if it hadn't converged?


## Second Step: Amp up settings and loop through optimx optimizers

Below we loop through some of **optimx**'s optimizer choices AND the better optimizer settings. These choices have worked for me in mixed models; for more info see the **optimx** documentation.

```{r cache = TRUE}
optimx_options <- c("L-BFGS-B", "nlminb", "nlm", "bobyqa", "nmkb", "hjkb")

for(i in 1:length(optimx_options)){
  model_flex <- lmer(y ~ arm + (1 + arm | clustid),
                     data = dat,
                     control = lmerControl(optimizer = "optimx",
                                           optCtrl = list(method = optimx_options[i],
                                                                   maxit = 1e9)))
  if(is.null(model_flex@optinfo$conv$lme4$messages)){
    print(paste0("One of the optimx options, ", optimx_options[i],", worked!"))
    print(summary(model_flex))
    break
  }
}
```



## Third Step: Amp up settings and loop through nloptwrap optimizers

The **nloptwrap** set of optimizers is a little easier to use, because it uses the same option control formatting as the default. It also has a large (20+) set of algorithm options, and it's easy to cycle through them in a for loop to find one that might work, all with the customized settings that you want. Note that here the first several options do NOT work, before settling on BOBYQA.

```{r cache = TRUE}
# More options are here: nloptr::nloptr.print.options() under "algorithm",
# but the set below seem to work most of the time.
algoptions <- c("NLOPT_LN_PRAXIS", "NLOPT_GN_CRS2_LM",
"NLOPT_LN_COBYLA", "NLOPT_LN_NEWUOA",
"NLOPT_LN_NEWUOA_BOUND", "NLOPT_LN_NELDERMEAD",
"NLOPT_LN_SBPLX", "NLOPT_LN_BOBYQA")

for(i in 1:length(algoptions)){
  model_flex <- lmer(y ~ arm + (1 + arm | clustid),
                     data = dat,
                     control = lmerControl(optimizer = "nloptwrap",
                                           optCtrl = list(algorithm = algoptions[i],
                                                          maxeval = 1e7,
                                                          xtol_abs = 1e-9,
                                                          ftol_abs = 1e-9)))
  if(is.null(model_flex@optinfo$conv$lme4$messages)){
    print(paste0("One of the nloptwrap options, ", algoptions[i],", worked!"))
    print(summary(model_flex))
    break
  }
}
```


# STILL no convergence?!?!

If none of the algorithms converge, you may not be stuck yet. In lme4's documentation (see ?lme4::convergence), they note that you may still be able to use the results of the non-converging models, though there is not universal agreement about that. Here are some links that could help in that situation and provide further info:

[Examining the log-likelihood, coefficients, and runtime of different algorithms](http://svmiller.com/blog/2018/06/mixed-effects-models-optimizer-checks/)


[Examples of other approaches to acheiving convergence besides just switching the algorithm](https://rstudio-pubs-static.s3.amazonaws.com/33653_57fc7b8e5d484c909b615d8633c01d51.html)


[Tricks for speeding up model fitting](https://cran.r-project.org/web/packages/lme4/vignettes/lmerperf.html)


[More general discussion of convergence, very illuminating](https://stats.stackexchange.com/questions/110004/how-scared-should-we-be-about-convergence-warnings-in-lme4)

