---
title: "Spline Notes for STAT 625 Project, Fall 2019"
subtitle: "Work in progress.."
author: "Josh Nugent"
output: html_document
---

Heavily indebted to various internet sources.

### Load useful packages etc...

```{r warning = F, message = F}
library(splines)
library(tidyverse)
library(mgcv)
library(pander)
knitr::opts_chunk$set(fig.height=3, fig.width=6, fig.align='center', warning = F, message = F)
set.seed(1)
```

# Linear Splines

### Mathematical model

Instead of a single regression line, we fit a set of piecewise linear regressions with the only restriction being that they intersect at the **knots**. Mathematically, with one predictor variable, we write the regression equation as follows. Note that we have K + 2 parameters to estimate.

$$
\begin{equation}
\tag{1}
E[y_i] = \beta_0 + \beta_1x_i+\sum_{k=1}^K b_k(x_i-\xi_k)_+ 
\end{equation}
$$

where

$$
\begin{equation}
\tag{2}
(x_i-\xi_k)_+ = 
\begin{cases} 
    0 & x_i < \xi_k \\
    (x_i-\xi_k) & x_i \geq \xi_k 
\end{cases}
\end{equation}
$$

where K is the number of knots. For a simple example, imagine you have one knot. Evaluating the sum, you can see that the term $b_k(x_i-\xi_k)$ will only change the expression for $x$-values greater than the knot. For those $x$-values above the knot, the regression function is $\mathbb{E}[y_i] = \beta_0 + \beta_1x_i + b_k(x_i-\xi_k)$, meaning there is an 'added amount of slope' $b_k$. Let's look at this visually:

```{r}
knot <- 3 # This is the location of our (in this case single) knot
x <- seq(from = 0, to = 6, by = .025)
y <- sin(2*x) + x -.1*x^2 + 2 + rnorm(length(x), sd = .3)

# More on this design matrix later...
design_matrix_X <- cbind(outer(x,1:1,"^"),outer(x,knot,">") * outer(x,knot,"-")^1)

mod_ls <- lm(y~design_matrix_X)
ggplot() +
  geom_point(aes(x = x, y = y), color = "black", alpha = .5) +
  geom_line(aes(x = x, y = predict(mod_ls)), color = "red") +
  geom_vline(aes(xintercept = knot), alpha = .5, linetype = "dotdash") +
  labs(title = "Piecewise linear spline... not a great fit!")
```

### Interpreting coefficients

Here are the coefficients from our model:

```{r}
summary(mod_ls)$coef
```

The intercept is... well, the intercept, and that matches our plot. The "design_matrix_X1" coefficient is the slope for the first section, while "design_matrix_X2" is the *additional* slope for the second section. hence, the second section has slope $.55 + -.6 = -.05$, which matches what we see in the plot.

Now let's connect what we did above to the design matrix...

### Design matrix, calculating coefficients

Earlier we used some slick R commands to make a design matrix that matched (1) and (2). Let's look at that matrix in detail. Note that it does not have a vector of $1$s for the intercept (yet).

```{r}
#design_matrix_X[1:nrow(design_matrix_X) %% 10==1,]
design_matrix_X[120:123,]
```

Only a subset is shown above. The first column is $x_1$ and the second column is 0 when $x_1$ is below the knot at $x=3$, and $(x_1-3)$ when $x_1$ is above the knot. Let's assemble the full design matrix and use LSE to estimate the coefficients:

```{r}
DMX <- cbind(1, design_matrix_X)
(betas <- solve(t(DMX) %*% DMX) %*% t(DMX) %*% y)
summary(mod_ls)$coef
```

They match what we expect.

# How many knots? Where to put?

Let's extend the same approach but use more knots:

```{r}
generate_design_matrix <- function(x, knot_vector, degree){
  return(cbind(outer(x,1:degree,"^"),outer(x,knot_vector,">")*outer(x,knot_vector,"-")^degree))
}
design_matrix2 <- generate_design_matrix(degree = 1, knot_vector = c(1,2.5,4, 5.7), x = x)
mod_ls2 <- lm(y~design_matrix2)
design_matrix3 <- generate_design_matrix(degree = 1, knot_vector = seq(from = 0.1, to = 5.9, by = .2), x = x)
mod_ls3 <- lm(y~design_matrix3)
yhatbad <- predict(mod_ls3)
ggplot() +
  geom_point(aes(x = x, y = y), color = "black", alpha = .5) +
  geom_line(aes(x = x, y = predict(mod_ls2)), color = "red") +
  geom_line(aes(x = x, y = yhatbad), color = "blue") +
  labs(title = "Piecewise linear spline - Good number vs. too many knots...")
```


# Alternative to choosing knot location: Penalize the $b_i$ coefficients for each piece

## Shrink coefficients via penalty term $\lambda$

Earlier, we noted that we are following the model below:

$$
\begin{equation}
\tag{1}
  E[y_i] = \beta_0 + \beta_1x_i+\sum_{k=1}^K b_k(x_i-\xi_k)_+
\end{equation}
$$

One way to keep the model from overfitting is to put a constraint on $\sum b_k^2$, say, $\sum b_k^2 < C$, to keep the coefficients from making the slopes leap up and down at each knot. Our penalty term $\lambda$ is a function of $C$.

Under this constraint, it can be shown (derivation omitted) that the OLS estimator for the coefficients is:

$$
\begin{equation}
\tag{4}
\hat{\boldsymbol{\beta}}=(\mathbf{X}^{T} \mathbf{X}+\lambda^{2} \mathbf{D})^{-1} \mathbf{X}^{T} \mathbf{y}
\end{equation}
$$

Where $\mathbf{D}$ is a matrix of all zeros, except on the diagonal, where the first two terms are zero (for $\beta_0$ and $\beta_1$) and the rest of the diagonal terms are 1 (for all of the constrained $b_k$s).

Using that result to investigate different penalty levels for our data set:

```{r}
x <- seq(from = 0, to = 6, by = .025)
knot_vector = seq(from = 0.1, to = 5.9, by = .2)
D <- diag(c(0,0,rep(1, times = length(knot_vector))))
X <- cbind(1, generate_design_matrix(degree = 1, knot_vector = knot_vector, x = x))
# derivation of coefficients for penalized linear splines
betas_.5 <- solve(t(X) %*% X + (.5^2) * D) %*% t(X) %*% y
betas_1 <- solve(t(X) %*% X + (1^2) * D) %*% t(X) %*% y
betas_3 <- solve(t(X) %*% X + (3^2) * D) %*% t(X) %*% y
yhat.5 <- X %*% betas_.5
yhat1 <- X %*% betas_1
yhat3 <- X %*% betas_3
ggplot() +
  geom_point(aes(x = x, y = y), color = "black", alpha = .3) +
  geom_line(aes(x = x, y = yhat.5), color = "black", alpha = 1) +
  geom_line(aes(x = x, y = yhat3), color = "green", alpha = 1) +
  geom_line(aes(x = x, y = yhatbad), color = "red", alpha = 1) +
  labs(title = "lambda = 0 (overfit), .5 (good), 3 (underfit)")
```

## Choosing $\lambda$ via cross-validation

As in many applications, we can use cross-validation to select the best balance of bias and variance to minimize the mean squared error. Here we'll do 5-fold CV to find a value of $lambda$ somewhere between 0 and 1. I'm also going to reduce the number of knots to make the matrix inversions possible.

```{r}
library(caret)

knot_vector = seq(from = 0.1, to = 5.9, by = .4)
D <- diag(c(0,0,rep(1, times = length(knot_vector))))

k <- 5
fold_indices <- createFolds(x, k = k)
lambda_grid <- seq(from = 0, to = 1, by = .01)
results <- matrix(data = NA, nrow = k, ncol = length(lambda_grid))

for(j in 1:length(lambda_grid)){
    for(i in 1:k){
      X_test <- cbind(1, generate_design_matrix(degree = 1, knot_vector = knot_vector, x = x[fold_indices[[i]]]))
      X_train <- cbind(1, generate_design_matrix(degree = 1, knot_vector = knot_vector, x = x[-fold_indices[[i]]]))
      betas_train <- solve(t(X_train) %*% X_train + (lambda_grid[j]^2) * D) %*% t(X_train) %*% y[-fold_indices[[i]]]
      y_test <- X_test %*% betas_train
      mse <- mean((y[fold_indices[[i]]] - y_test)^2)
      results[i,j] <- mse
    }
}
CV_mse <- colMeans(results)

ggplot() +
  geom_line(aes(x = lambda_grid, y = CV_mse), color = "black", alpha = 1) +
  labs(title = "MSE", y = "Average MSE across the 5 folds", x = "lambda value")
```

The lowest MSE occurrs when $\lambda$ is about $.25$ Let's fit the whole data set with that value and examine the results:

```{r}
D <- diag(c(0,0,rep(1, times = length(knot_vector))))
X <- cbind(1, generate_design_matrix(degree = 1, knot_vector = knot_vector, x = x))
betas_CV <- solve(t(X) %*% X + (.25^2) * D) %*% t(X) %*% y
yhatCV <- X %*% betas_CV
ggplot() +
  geom_point(aes(x = x, y = y), color = "black", alpha = .3) +
  geom_line(aes(x = x, y = yhatCV), color = "black", alpha = 1) +
  labs(title = "lambda = .25... looks like a pretty good fit for these knot choices.")
```


An obvious downside of the above is that most real-world processes don't have the sharp changes in slope implied by the model fit above. In fact, linear splines are a pretty crude way to model a non-linear relationship, so we're going to move to a more advanced technique. However, the reason we went through the linear spline example in such detail is that the big ideas (building the design matrix, estimation, and penalization for over/underfitting) are almost the same, but it's easier to build the intuition with the simple linear case.

# Polynomial Splines

Polynomial splines address the concern of jagged splines by using piecewise polynomials (usually of degree 3) that connect at the knot points and *whose derivatives also match at the knot points*, making for a smooth curve through the data. The same concerns of over/underfitting are present 

### Mathematical model

A linear spline is of course a special case of the more general polynomial spline, where the sections between knots are polynomials of degree $D$. The formula below is equivalent to the linear spline formula, but with added polynomial terms. We now have D + K + 1 parameters to estimate.

$$
\begin{equation}
\tag{5}
E[y_i]=\beta_{0}+\beta_{1} x_i+\ldots \beta_{D} x_i^{D}+\sum_{k=1}^{K} b_{k}\left(x_i-\xi_{k}\right)_{+}^{D}
\end{equation}
$$

We generate the design matrix in the same way, just with many more terms. Apply to our data set:

```{r}
X <- cbind(1, generate_design_matrix(degree = 3, knot_vector = c(2), x = x))
betas <- solve(t(X) %*% X) %*% t(X) %*% y
yhat <- X %*% betas
ggplot() +
  geom_point(aes(x = x, y = y), color = "black", alpha = .3) +
  geom_line(aes(x = x, y = yhat), color = "black", alpha = 1) +
  geom_vline(aes(xintercept = 2), color = "black", linetype = "dotdash") +
  labs(title = "Cubic spline, 1 knot at 2, no penalization",
       subtitle = "Because of derivative-matching constraint, it's underfitting")

X <- cbind(1, generate_design_matrix(degree = 3, knot_vector = seq(.2, 5.8, .2), x = x))
betas <- solve(t(X) %*% X) %*% t(X) %*% y
yhat <- X %*% betas
ggplot() +
  geom_point(aes(x = x, y = y), color = "black", alpha = .3) +
  geom_line(aes(x = x, y = yhat), color = "black", alpha = 1) +
  geom_vline(aes(xintercept = seq(.2, 5.8, .2)), color = "black", alpha = .5, linetype = "dotdash") +
  labs(title = "Cubic spline",
       subtitle = "Too many knots, no penalization, overfitting!")
```

We can apply the same cross-validated penalization routine to get an appropriate fit to our data. Note that we can also increase the degree of the polynomial to add flexibility to the model, but the danger of overfitting is even more pronounced.

```{r}
# We haven't explained the "bs" function yet, stay tuned...
modbs <- lm(y~bs(x, knots = c(3), degree = 50))
ggplot() + geom_point(aes(x = x, y = y), color = "black", alpha = .3) +
  geom_vline(aes(xintercept = knot), alpha = .5, linetype = "dotdash") +
  geom_line(aes(x = x, y = predict(modbs)), color = "red") +
  labs(title = "Only one knot... but 50th degree polynomials obviously overfit!",
       subtitle = "Also, note that the function is less smooth far from the knot")
```



### Using B-splines instead of cubic splines as shown above

Depending on the data set, making the design matrix for a bunch of cubed $x_i$ values can lead to some very large (and very small) values and make the fitting algorithm unstable. Also, the spline can get very wiggly and unstable around the edges, as there are no knots to anchor it beyond the range of our data.






```
x <- runif(n = 200, max = 10)
y <- sin(2*x) + x -.1*x^2 + 2 + rnorm(length(x), sd = .3)
knots <- c(1, 4)
basis <- bs(x, knots = knots)
dat <- cbind.data.frame(y, basis)
mod <- lm(y~basis, data = dat)
ggplot() + geom_point(aes(x = x, y = y), color = "black") +
  geom_line(aes(x = x, y = predict(mod)), color = "red") +
  geom_vline(aes(xintercept = knots))

knots <- c(1, 7)
basis <- bs(x, knots = knots)
dat <- cbind.data.frame(y, basis)
mod <- lm(y~basis, data = dat)
ggplot() + geom_point(aes(x = x, y = y), color = "black") +
  geom_line(aes(x = x, y = predict(mod)), color = "red") +
  geom_vline(aes(xintercept = knots))

extrapX <- seq(10, 20, by = .1)
basis <- bs(extrapX, knots = knots)
extrapY <- predict(mod, newdata = basis)
ggplot() + geom_point(aes(x = x, y = y), color = "black") +
  geom_line(aes(x = extrapX, y = extrapY), color = "red") +
  geom_vline(aes(xintercept = knots)) +
  labs(title="EXTRAPOLATION - NOT CURRENTLY CORRECT")

```











