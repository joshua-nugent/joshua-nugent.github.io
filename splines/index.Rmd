---
title: "Spline Notes"
subtitle: "draft / work in progress!"
author: "Josh Nugent"
output: html_document
---

### Load useful packages etc...

```{r warning = F, message = F}
library(splines)
library(tidyverse)
library(mgcv)
library(pander)
knitr::opts_chunk$set(fig.height=3, fig.width=6, fig.align='center', warning = F, message = F)
```

# Linear Splines

### Mathematical model

Instead of a single regression line, we fit a set of piecewise linear regressions with the only restriction being that they intersect at the **knots**. Mathematically, with one predictor variable, we write the regression equation as follows:

$$
\begin{equation}
\tag{1}
E[y_i] = \beta_0 + \beta_1x_i+\sum_{k=1}^K b_k(x_i-\xi_k)_+ 
\end{equation}
$$

where

$$
\begin{equation}
\tag{2}
(x_i-\xi_k)_+ = 
\begin{cases} 
    0 & x_i < \xi_k \\
    (x_i-\xi_k) & x_i \geq \xi_k 
\end{cases}
\end{equation}
$$

where K is the number of knots. For a simple example, imagine you have one knot. Evaluating the sum, you can see that the term $b_k(x_i-\xi_k)$ will only change the expression for $x$-values greater than the knot. For those $x$-values above the knot, the regression function is $\mathbb{E}[y_i] = \beta_0 + \beta_1x_i + b_k(x_i-\xi_k)$, meaning there is an 'added amount of slope' $b_k$. Let's look at this visually:

```{r}
knot <- 3 # This is the location of our (in this case single) knot
x <- seq(from = 0, to = 6, by = .025)
y <- sin(2*x) + x -.1*x^2 + 2 + rnorm(length(x), sd = .3)

# More on this design matrix later...
design_matrix_X <- cbind(outer(x,1:1,"^"),outer(x,knot,">") * outer(x,knot,"-")^1)

mod_ls <- lm(y~design_matrix_X)
ggplot() +
  geom_point(aes(x = x, y = y), color = "black", alpha = .5) +
  geom_line(aes(x = x, y = predict(mod_ls)), color = "red") +
  geom_vline(aes(xintercept = knot), alpha = .5, linetype = "dotdash") +
  labs(title = "Piecewise linear spline... not a great fit!")
```

### Interpreting coefficients

Here are the coefficients from our model:

```{r}
summary(mod_ls)$coef
```

The intercept is... well, the intercept, and that matches our plot. The "design_matrix_X1" coefficient is the slope for the first section, while "design_matrix_X2" is the *additional* slope for the second section. hence, the second section has slope $.55 + -.6 = -.05$, which matches what we see in the plot.

Now let's connect what we did above to the design matrix...

### Design matrix, calculating coefficients

Earlier we used some slick R commands to make a design matrix that matched (1) and (2). Let's look at that matrix in detail. Note that it does not have a vector of $1$s for the intercept (yet).

```{r}
#design_matrix_X[1:nrow(design_matrix_X) %% 10==1,]
design_matrix_X[120:123,]
```

Only a subset is shown above. The first column is $x_1$ and the second column is 0 when $x_1$ is below the knot at $x=3$, and $(x_1-3)$ when $x_1$ is above the knot. Let's assemble the full design matrix and use LSE to estimate the coefficients:

```{r}
DMX <- cbind(1, design_matrix_X)
(betas <- solve(t(DMX) %*% DMX) %*% t(DMX) %*% y)
summary(mod_ls)$coef
```

They match what we expect.

# How many knots? Where to put?

Let's extend the same approach but use more knots:

```{r}
generate_design_matrix <- function(x, knot_vector, degree){
  return(cbind(outer(x,1:degree,"^"),outer(x,knot_vector,">")*outer(x,knot_vector,"-")^degree))
}
design_matrix2 <- generate_design_matrix(degree = 1, knot_vector = c(1,2.5,4, 5.7), x = x)
mod_ls2 <- lm(y~design_matrix2)
design_matrix3 <- generate_design_matrix(degree = 1, knot_vector = seq(from = 0.1, to = 5.9, by = .2), x = x)
mod_ls3 <- lm(y~design_matrix3)
yhatbad <- predict(mod_ls3)
ggplot() +
  geom_point(aes(x = x, y = y), color = "black", alpha = .5) +
  geom_line(aes(x = x, y = predict(mod_ls2)), color = "red") +
  geom_line(aes(x = x, y = yhatbad), color = "blue") +
  labs(title = "Piecewise linear spline - Good number vs. too many knots...")
```


# Alternative to choosing knot location: Penalize the $b_i$ coefficients for each piece

## Shrink coefficients via penalty term $\lambda$

Earlier, we noted that we are following the model below:

$$
\begin{equation}
\tag{1}
  E[y_i] = \beta_0 + \beta_1x_i+\sum_{k=1}^K b_k(x_i-\xi_k)_+
\end{equation}
$$

One way to keep the model from overfitting is to put a constraint on $\sum b_k^2$, say, $\sum b_k^2 < C$, to keep the coefficients from making the slopes leap up and down at each knot. Our penalty term $\lambda$ is a function of $C$.

Under this constraint, it can be shown (derivation omitted) that the OLS estimator for the coefficients is:

$$
\hat{\boldsymbol{\beta}}=(\mathbf{X}^{T} \mathbf{X}+\lambda^{2} \mathbf{D})^{-1} \mathbf{X}^{T} \mathbf{y}
$$

Using that result to investigate 

```{r}
x <- seq(from = 0, to = 6, by = .025)
knot_vector = seq(from = 0.1, to = 5.9, by = .2)
D <- diag(c(0,0,rep(1, times = length(knot_vector))))
X <- cbind(1, generate_design_matrix(degree = 1, knot_vector = knot_vector, x = x))
# derivation of coefficients for penalized linear splines
betas_.5 <- solve(t(X) %*% X + (.5^2) * D) %*% t(X) %*% y
betas_1 <- solve(t(X) %*% X + (1^2) * D) %*% t(X) %*% y
betas_3 <- solve(t(X) %*% X + (3^2) * D) %*% t(X) %*% y
yhat.5 <- X %*% betas_.5
yhat1 <- X %*% betas_1
yhat3 <- X %*% betas_3
ggplot() +
  geom_point(aes(x = x, y = y), color = "black", alpha = .3) +
  geom_line(aes(x = x, y = yhat.5), color = "black", alpha = 1) +
  geom_line(aes(x = x, y = yhat3), color = "green", alpha = 1) +
  geom_line(aes(x = x, y = yhatbad), color = "red", alpha = 1) +
  labs(title = "lambda = 0, .5, 3")
```

## Choosing $\lambda$ via cross-validation









# Polynomial Splines

### Mathematical model

A linear spline is of course a special case of the more general polynomial spline, where the sections between knots are polynomials of degree $D$.



### Problem: The Edges

If you've ever seen a Taylor series in Calculus class or fit a polynomial function to a set of points, you have probably seen how polynomials can really freak out around the edges.


Big idea: We need piecewise cubic polynomials that minimize error but also have matching 1st and 2nd derivatives. I think the format of the cubic basis

Cubic splines microexample?

```{r}
x <- 0:6
knot <- 3
D <- 2

(X <- cbind(outer(x,1:D,"^"),outer(x,knot,">") * outer(x,knot,"-")^D))

knot2 <- c(2, 4)
(X2 <- cbind(outer(x,1:D,"^"),outer(x,knot2,">") * outer(x,knot2,"-")^D))

D3 <- 3
(X3 <- cbind(outer(x,1:D3,"^"),outer(x,knot,">") * outer(x,knot,"-")^D3))

(X23 <- cbind(outer(x,1:D3,"^"),outer(x,knot2,">") * outer(x,knot2,"-")^D3))


x <- seq(from = 0, to = 6, by = .025)
y <- sin(2*x) + x -.1*x^2 + 2 + rnorm(length(x), sd = .3)
X <- cbind(outer(x,1:D,"^"),outer(x,knot,">") * outer(x,knot,"-")^D)

mod2 <- lm(y ~ X) #summary(mod2)
modbs <- lm(y~bs(x, knots = knot, degree = 2))
ggplot() + geom_point(aes(x = x, y = y), color = "black", alpha = .3) +
  geom_line(aes(x = x, y = predict(mod2)), color = "red") +
  geom_vline(aes(xintercept = knot), alpha = .5, linetype = "dotdash") +
  geom_line(aes(x = x, y = predict(modbs)), linetype = "dotdash")

knot <- c(2, 5)
ggplot() + geom_point(aes(x = x, y = y), color = "black", alpha = .3) +
  geom_vline(aes(xintercept = knot), alpha = .5, linetype = "dotdash") +
  geom_line(aes(x = x, y = predict(lm(y~bs(x, knots = knot, degree = 2)))), color = "red")

knot <- c(1, 3, 5)
ggplot() + geom_point(aes(x = x, y = y), color = "black", alpha = .3) +
  geom_vline(aes(xintercept = knot), alpha = .5, linetype = "dotdash") +
  geom_line(aes(x = x, y = predict(lm(y~bs(x, knots = knot, degree = 2)))), color = "red")

knot <- c(1, 3, 5)
ggplot() + geom_point(aes(x = x, y = y), color = "black", alpha = .3) +
  geom_vline(aes(xintercept = knot), alpha = .5, linetype = "dotdash") +
  geom_line(aes(x = x, y = predict(lm(y~bs(x, knots = knot, degree = 30)))), color = "red") +
  labs(title = "bad... polynomials in between knots have too high a degree, overfitting")

knot <- seq(from=.5, to = 5.5, by = .2)
ggplot() + geom_point(aes(x = x, y = y), color = "black", alpha = .3) +
  geom_vline(aes(xintercept = knot), alpha = .5, linetype = "dotdash") +
  geom_line(aes(x = x, y = predict(lm(y~bs(x, knots = knot, degree = 2)))), color = "red") +
  labs(title = "bad... too many knots, overfitting")

```



```{r}
x <- runif(n = 200, max = 10)
y <- sin(2*x) + x -.1*x^2 + 2 + rnorm(length(x), sd = .3)
knots <- c(1, 4)
basis <- bs(x, knots = knots)
dat <- cbind.data.frame(y, basis)
mod <- lm(y~basis, data = dat)
ggplot() + geom_point(aes(x = x, y = y), color = "black") +
  geom_line(aes(x = x, y = predict(mod)), color = "red") +
  geom_vline(aes(xintercept = knots))

knots <- c(1, 7)
basis <- bs(x, knots = knots)
dat <- cbind.data.frame(y, basis)
mod <- lm(y~basis, data = dat)
ggplot() + geom_point(aes(x = x, y = y), color = "black") +
  geom_line(aes(x = x, y = predict(mod)), color = "red") +
  geom_vline(aes(xintercept = knots))

extrapX <- seq(10, 20, by = .1)
basis <- bs(extrapX, knots = knots)
extrapY <- predict(mod, newdata = basis)
ggplot() + geom_point(aes(x = x, y = y), color = "black") +
  geom_line(aes(x = extrapX, y = extrapY), color = "red") +
  geom_vline(aes(xintercept = knots)) +
  labs(title="EXTRAPOLATION - NOT CURRENTLY CORRECT")

```




# Controlling overfitting / underfitting via CV

Cross-validation is a general tool for tuning models to not be too simple (underfit, with high bias and low variance) or too complex (overfit, with low bias and high variance).

The general idea is to fit models of varying complexity on subsets of the data, then test them on the left-out sets. The models that have the 'best' level of complexity will have the least error on the held-out test data.








